{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPS1x3EuhIfIfm0zHjpDiHb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries and Configuration\n"
      ],
      "metadata": {
        "id": "Ezqi-IGxg_eP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQkaujrCg7XD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    pipeline\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset from KaggleHub"
      ],
      "metadata": {
        "id": "7_DDpxnahGWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "csv_path = os.path.join(path, \"IMDB Dataset.csv\")\n",
        "data = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "cxYKEns0hGsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(data.head())"
      ],
      "metadata": {
        "id": "Lcia39iPhRRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(data.tail())"
      ],
      "metadata": {
        "id": "Q4KvN_zAhUeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "_Bx6J0TfhiYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data)"
      ],
      "metadata": {
        "id": "3yNdMzQYhoWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "id": "UpVDkI-Nhr67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label Encoding for Sentiment Classes"
      ],
      "metadata": {
        "id": "XzGNT0e3h25p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.replace({\"sentiment\": {\"positive\": 1, \"negative\": 0}}, inplace=True)"
      ],
      "metadata": {
        "id": "mWBRh7R5h1cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "cyp0_fGxhyX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "INdOz7jahy4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data"
      ],
      "metadata": {
        "id": "2Nu0BVosiF2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data[\"review\"].tolist(), data[\"sentiment\"].tolist(), test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "n5KRd4W9iDGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "L8KWAkYq0jA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing (Tokenization)"
      ],
      "metadata": {
        "id": "UF_NjZBriMwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common preprocessing function\n",
        "def preprocess_data(tokenizer):\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "    val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
        "    test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n",
        "\n",
        "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    return tokenized_train,tokenized_val, tokenized_test"
      ],
      "metadata": {
        "id": "_iQ-7Pbck81m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Computation Function"
      ],
      "metadata": {
        "id": "G6-apyg8Iulz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shared compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "ij_DnsEtk-m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Each Model Separately and Collect Results\n",
        "### Model 1: nikolasmoya/imdb-binary-sentiment-analysis"
      ],
      "metadata": {
        "id": "SwOpXO4olW1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = list()\n",
        "\n",
        "# Train imdb model\n",
        "start_imdb = time.time()\n",
        "nikolasmoya_model_name = \"nikolasmoya/imdb-binary-sentiment-analysis\"\n",
        "nikolasmoya_tokenizer = AutoTokenizer.from_pretrained(nikolasmoya_model_name)\n",
        "nikolasmoya_model = AutoModelForSequenceClassification.from_pretrained(nikolasmoya_model_name)\n",
        "\n",
        "nikolasmoya_tokenized_train, nikolasmoya_tokenized_eval, nikolasmoya_tokenized_test = preprocess_data(nikolasmoya_tokenizer)\n",
        "nikolasmoya_data_collator = DataCollatorWithPadding(tokenizer=nikolasmoya_tokenizer)\n",
        "\n",
        "nikolasmoya_training_args = TrainingArguments(\n",
        "output_dir=\"./results/imdb\",\n",
        "learning_rate=2e-5,\n",
        "per_device_train_batch_size=16,\n",
        "per_device_eval_batch_size=16,\n",
        "num_train_epochs=2,\n",
        "weight_decay=0.01,\n",
        "logging_dir='./logs'\n",
        ")\n",
        "\n",
        "nikolasmoya_trainer = Trainer(\n",
        "    model=nikolasmoya_model,\n",
        "    args=nikolasmoya_training_args,\n",
        "    train_dataset=nikolasmoya_tokenized_train,\n",
        "    eval_dataset=nikolasmoya_tokenized_eval,\n",
        "    tokenizer=nikolasmoya_tokenizer,\n",
        "    data_collator=nikolasmoya_data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "nikolasmoya_trainer.train()\n",
        "nikolasmoya_results = nikolasmoya_trainer.evaluate()\n",
        "print(\"IMDB Model Results:\")\n",
        "print(nikolasmoya_results)\n",
        "nikolasmoya_preds = np.argmax(nikolasmoya_trainer.predict(nikolasmoya_tokenized_test).predictions, axis=-1)\n",
        "print(classification_report(test_labels, nikolasmoya_preds, target_names=[\"Negative\", \"Positive\"]))\n",
        "end_imdb = time.time()\n",
        "nikolasmoya_results[\"model\"] = \"nikolasmoya/imdb-binary-sentiment-analysis\"\n",
        "nikolasmoya_results[\"time_sec\"] = end_imdb - start_imdb\n",
        "results.append(nikolasmoya_results)"
      ],
      "metadata": {
        "id": "P2624UvpzFW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: kubi565/roberta-binary-sentiment-multilingual"
      ],
      "metadata": {
        "id": "7FargNT2JCNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train roberta model\n",
        "start_roberta = time.time()\n",
        "roberta_model_name = \"kubi565/roberta-binary-sentiment-multilingual\"\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_name)\n",
        "\n",
        "roberta_tokenized_train, roberta_tokenized_eval ,roberta_tokenized_test = preprocess_data(roberta_tokenizer)\n",
        "roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
        "\n",
        "roberta_training_args = TrainingArguments(\n",
        "output_dir=\"./results/roberta\",\n",
        "learning_rate=2e-5,\n",
        "per_device_train_batch_size=16,\n",
        "per_device_eval_batch_size=16,\n",
        "num_train_epochs=2,\n",
        "weight_decay=0.01,\n",
        "logging_dir='./logs'\n",
        ")\n",
        "\n",
        "roberta_trainer = Trainer(\n",
        "    model=roberta_model,\n",
        "    args=roberta_training_args,\n",
        "    train_dataset=roberta_tokenized_train,\n",
        "    eval_dataset=roberta_tokenized_eval,\n",
        "    tokenizer=roberta_tokenizer,\n",
        "    data_collator=roberta_data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "roberta_trainer.train()\n",
        "roberta_results = roberta_trainer.evaluate()\n",
        "print(\"roberta Results:\")\n",
        "print(roberta_results)\n",
        "roberta_preds = np.argmax(roberta_trainer.predict(roberta_tokenized_test).predictions, axis=-1)\n",
        "print(classification_report(test_labels, roberta_preds, target_names=[\"Negative\", \"Positive\"]))\n",
        "end_roberta = time.time()\n",
        "roberta_results[\"model\"] = \"roberta-binary-sentiment-multilingual\"\n",
        "roberta_results[\"time_sec\"] = end_roberta - start_roberta\n",
        "results.append(roberta_results)"
      ],
      "metadata": {
        "id": "tuMl5xkfqnDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: autoevaluate/binary-classification"
      ],
      "metadata": {
        "id": "BgY6tYxcJILr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train binary_classification model\n",
        "start_binary_classification = time.time()\n",
        "binary_classification_model_name = \"autoevaluate/binary-classification\"\n",
        "binary_classification_tokenizer = AutoTokenizer.from_pretrained(binary_classification_model_name)\n",
        "binary_classification_model = AutoModelForSequenceClassification.from_pretrained(binary_classification_model_name)\n",
        "\n",
        "binary_classification_tokenized_train,binary_classification_tokenized_eval, binary_classification_tokenized_test = preprocess_data(binary_classification_tokenizer)\n",
        "binary_classification_data_collator = DataCollatorWithPadding(tokenizer=binary_classification_tokenizer)\n",
        "\n",
        "binary_classification_training_args = TrainingArguments(\n",
        "output_dir=\"./results/binary-classification\",\n",
        "learning_rate=2e-5,\n",
        "per_device_train_batch_size=16,\n",
        "per_device_eval_batch_size=16,\n",
        "num_train_epochs=2,\n",
        "weight_decay=0.01,\n",
        "logging_dir='./logs'\n",
        ")\n",
        "\n",
        "binary_classification_trainer = Trainer(\n",
        "    model=binary_classification_model,\n",
        "    args=binary_classification_training_args,\n",
        "    train_dataset=binary_classification_tokenized_train,\n",
        "    eval_dataset=binary_classification_tokenized_eval,\n",
        "    tokenizer=binary_classification_tokenizer,\n",
        "    data_collator=binary_classification_data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "binary_classification_trainer.train()\n",
        "binary_classification_results = binary_classification_trainer.evaluate()\n",
        "print(\"Binary Classification Results:\")\n",
        "print(binary_classification_results)\n",
        "binary_classification_preds = np.argmax(binary_classification_trainer.predict(binary_classification_tokenized_test).predictions, axis=-1)\n",
        "print(classification_report(test_labels, binary_classification_preds, target_names=[\"Negative\", \"Positive\"]))\n",
        "end_binary_classification = time.time()\n",
        "binary_classification_results[\"model\"] = \"binary-classification\"\n",
        "binary_classification_results[\"time_sec\"] = end_binary_classification - start_binary_classification\n",
        "results.append(binary_classification_results)"
      ],
      "metadata": {
        "id": "AkEMbqI4quLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Comparison (Benchmarking)"
      ],
      "metadata": {
        "id": "xISgWsIsJQYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n Benchmark Summary:\\n\")\n",
        "display(df_results)"
      ],
      "metadata": {
        "id": "8e_CIRPYluwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix Visualization"
      ],
      "metadata": {
        "id": "-tpjHuvrJn0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Confusion Matrix visualization\n",
        "def plot_conf_matrix(true, pred, model_name):\n",
        "    cm = confusion_matrix(true, pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.show()\n",
        "\n",
        "plot_conf_matrix(test_labels, nikolasmoya_preds, \"nikolasmoya/imdb-binary-sentiment-analysis\")\n",
        "plot_conf_matrix(test_labels, roberta_preds, \"kubi565/roberta-binary-sentiment-multilingual\")\n",
        "plot_conf_matrix(test_labels, binary_classification_preds, \"autoevaluate/binary-classification\")"
      ],
      "metadata": {
        "id": "vpDMtJEX6gM0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}